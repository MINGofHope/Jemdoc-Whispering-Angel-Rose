# jemdoc: menu{outline/ML}{ml.html}

= Linear Regression
Suppose that we have data points $D = \{(x_1, y_1), \ldots, (x_i, y_i), \ldots, (x_n, y_n)\}$, ~$x_i \in \mathbb{R^{d}}$,
and $y_i \in \mathbb{R}$. ~$X_{n*p} = (x_1, x_2, \ldots, x_n)^T$, ~$Y_{n*1}=(y_1, \ldots, y_n)^T$.

~~~
{Task}
to find a optimal $\theta^{*}_{(d+1)*1}$ that form the hypothesis function $h(x) = \sum_{j=0}^{d}\theta^jx_i^j = \theta^Tx_i$,
to approximate the real function $f(x)$ that represents the intrinsic pattern of the data.
~~~

~~~
{Loss/Cost Function}
To quantify the sum of the difference between the predict value $\hat{y_i}$ and the real one $y_i$,
\(
    L(Y, \hat{Y}) = \sum_{i=1}^{n} L(y_i, \hat{y_i}) = \sum_{i=1}^{n} L(y_i, \theta^Tx_i)
\)
~~~

~~~
{Solve Parameters}
To minimize the Loss Function,
\(
    \theta^{\star} = \underset{\theta}{argmax}(\sum_{i=1}^{n} L(y_i, \theta^Tx_i))
\)
~~~

== Least Squares Estimation (LSE)
=== Method 1: Normal Equation
For this problem, we use the squared error defined by the L2 norm to define the loss function.

\(
    L(\theta) = \sum_{i=1}^{n}(\theta^Tx_i - y_i)^{2} = \sum_{i=1}^{n}||\theta^Tx_i - y_i||^{2}_{2} = ||X\theta - Y||^2_2 \\

\)

Let $\frac{\partial L}{\partial \theta} = 0$,

\(
\begin{equation*}
\begin{split}
    \frac{\partial L}{\partial \theta} &= \frac{\partial [(X\theta - Y)^T(X\theta - Y)]}{\partial \theta} \\
    & = \frac{\partial [\theta^TX^TX\theta - \theta^TX^TY - Y^TX\theta + Y^TY]}{\partial \theta} \\
    & = \frac{\partial [\theta^TX^TX\theta - 2\theta^TX^TY + Y^TY]}{\partial \theta} \\
    & = 2X^TX\theta - 2X^TY \\
    & = 0
\end{split}
\end{equation*}
\)

The close-form solution would be,
~~~
\(
\theta = (X^TX)^{-1}(X^TY)
\)
~~~

==== Assumption $\rightarrow$ $(X^TX)^{-1}$ is invertible
~~~
Let $A = X^TX_{p*p}$, a square matrix, is invertible $\iff$ $A$ is full rank.
~~~

Prove: A is invertible $\iff$ $A^{-1}A = AA^{-1} = I$, i.e. $A[a_1, a_2, \ldots, a_p] = I = [I_1, I_2, \ldots, I_p]$ $\rightarrow$
$A \cdot a_1 = I_1$ has unique solution $\iff$ There are pth equations and pth unknown factor for solving $a_1$. Besides,
all rows or columns in $A$ is linear independent (full rank). If you are not familiar with terms like linear independent,
or rank here, pls refer to section - Math Basics.

~~~
A is full rank $\rightarrow$ $X_{n*p}$, $n >> p$
~~~

*Prove*: Rank(A) $<= min(min($X^T$), min(X)) <= min(X) = min(n, p)$; If $n >> p$, then $Rank(A_{p*p}) = p <= min(n,p)$,
Otherwise $n << p$, Rank($A_{p*p}$) $= p <= min(n, p) = n$, so this contradicts the assumption. The assumption is wrong.

==== Assumption: $X$ is not large, i.e., $n <= n^{*}, d < d^{*}$

~~~
It is costly to calculate the inverse of a large matrix, so we usually choose this method when the size of data is relative
small, i.e., the number of records and features.
~~~

In a nutshell, only $X^TX$ is invertible, and the number of data records is not that high (should be greater than the
number of features but less than $n^{\star} = 10000$ (arbitrarily))

=== Method 2: Gradient Descent
If there is no inverse for $X^TX$ and X is a huge matrix, *Gradient Descent* (GD) is employed for solving the optimal $\theta$.
We know the loss function $L(\theta) = ||X\theta - Y||_2^2$ is a *convex function* (think the shape of the curve along the
time), so it has a unique global minimum for $\theta$. The intuition for GD is to go down along the curve to the bottom
point in the fastest direction, which is the opposite of gradient at that position, $-\frac{\partial L}{\partial \theta^{t}}$.
How far we should move is determined by the step size (learning rate, $\eta$). Update vector is the product of these two
items, $-\eta\frac{\partial L}{\partial \theta^{t}}$. For each iteration, we update the $\theta$ as follows,

\(
    \theta^{t+1} = \theta^{t} - \eta\frac{\partial L}{\partial \theta^{t}}
\)

To address, the update vector is not necessarily in the same direction as the theta. The update vector determines the
change in theta, and the magnitude of the change depends on the learning rate and the size of the gradient. Substitute
the partial part the update formula,

~~~
\(
    \theta^{t+1} = \theta^{t} - \eta \cdot X^T(X\theta^{t} - Y)
\)
~~~

Actually, $\frac{\partial L}{\partial \theta^t} = 2(X^TX\theta^t - X^TY)$, and the scale factor 2 can be incorporated into
$\eta$. Also, Make sure that you really understand the variables in the above formula, so that you can calculate the updated
$\theta$ manually given a data set. You can have an intuitive understanding for each variable by analyzing the dimensionality.
Here is the reference, $\theta_{p*1}, \eta \in \mathbb{R}, X_{n*d}, Y_{n*1}$. Then, you for loop iterations and update
the $\theta$ until convergence as follows,

~~~
{Pseudocode}{pseudocode}
0. initialize \theta by creating an array with the size of p.
1. for i in range(n_iterations):
    1.1 update the \theta using the above formula
    1.2 judge if convergence conditions are met
        1.2.1 if so, break the loop
        1.2.2 otherwise, go to next loop
~~~

Four convergence conditions are introduced.
. If the sum of square of gradient difference between the time $t$ and $t+1$ is lower than the arbitrary limit,
$||(\frac{\partial L}{\partial \theta})^{t+1} - (\frac{\partial L}{\partial \theta})^{t+1}||_2^2 <= \epsilon$

. If the loss change lower than the limit,
$L(\theta)^{t+1} - L(\theta)^{t} <= \epsilon$\

. If the sum of the square of difference between $\theta$ at times of $t+1$ and $t$ is lower than the arbitrary limit,
$||\theta^{(t+1)} - \theta^{(t)}||_2^2 <= \epsilon$

. Plot the loss values or validation accuracy along iteration. If the curve approaches to plain, stop.

==== Stochastic Gradient Descent
Stochastic gradient descent (SGD) is a variant of gradient descent optimization algorithm that randomly selects a
subset of data points from the entire dataset to compute the gradient of the loss function and updates the model
parameters based on the computed gradient. In other words, instead of computing the gradient over the entire dataset,
SGD computes the gradient over a randomly selected subset of data points, which makes it faster and more efficient for
large datasets. The stochasticity in SGD comes from the random selection of data points, which introduces some noise
into the optimization process, but can also help the algorithm to escape from local minima.

~~~
{Pseudocode}{pseudocode}
0. initialize \theta by creating an array with the size of p.
1. for i in range(n_iterations):
    1.0 *randomly pick the subset of the original data*
    1.1 update the \theta using the above formula
    1.2 judge if convergence conditions are met
        1.2.1 if so, break the loop
        1.2.2 otherwise, go to next loop
~~~

== Maximum Likelihood Estimation (MLE)
Except for LSE, we can start from the probability to solve the task in linear regression, to find a hypothesis function
$\hat{y_i} = h(x_i) = \theta^Tx_i$ with a small noise function, $\epsilon' \sim \mathcal{N}(0, \sigma^2)$ , to approximate
$f(x_i)=y_i$ of the data. That is,
\(
    y_i = \theta^Tx_i + \epsilon' \sim \mathcal{N}(\theta^Tx_i, \sigma^2)
\)

The idea is to maximize the likelihood function, $p(Y|\theta)$,

\(
\begin{equation*}
\begin{split}
    p(Y|\theta) &= \prod_{i=1}^{n} p(y_i|\theta) \\
    & = \prod_{i=1}^n \frac{1}{\sqrt{2\pi}\sigma}\exp(-\frac{(y_i - \theta^Tx_i)^2}{\sigma^2}) \\
    & \underset{log}{\rightarrow} \sum_{i=1}^{n} -(y_i - \theta^Tx_i)^2\\
    & = -L_{LSE}
\end{split}
\end{equation*}
\)

To solve $\theta^{\star}$, we can adopt normal equation or gradient descent method, and the interesting thing is,
~~~
Maximize the likelihood function equals to minimize the least square error. MLE and LSE say the same story for linear
regression from different perspectives.
~~~

== Maximum A Posterior (MAP)
We think the task of linear regression in a bayesian perceptive, model on the probability of $\theta$ given $Y$ (posterior),
and maximize it for the optimal $\theta$. Suppose prior follows a normal distribution with a mean value of 0, $p(\theta)
\sim \mathcal{N}(0, \sigma_{0}^2)$, then $\theta^{*}$ will be calculated,

\(
\begin{equation*}
\begin{split}
    \theta^{*} & = \underset{\theta}{argmax} logp(\theta|Y) \\
    &= \underset{\theta}{argmax} log[p(Y|\theta)p(\theta)] \\
    & = \underset{\theta}{argmax} [log(p(Y|\theta) + logp(\theta)]\\
    & \rightarrow \underset{\theta}{argmin} [(X\theta - Y)^2 + \frac{\sigma^2}{\sigma_{0}^2}\theta^T\theta]\\
\end{split}
\end{equation*}
\)

== Current Issues
=== No Bias
We only use a line that cross the origin point to approximate the data, i.e, $y=\theta^Tx$, however, it can not model
those data of which the pattern should be $y = \theta^Tx + b$. For calculation simplification, we incorporate the bias
into $\theta_{p*1}$, and add a bias 1, $1$, to the $x_{p*1}$. So, the new $\theta'$, and $x'$ that
support the bias item are shown below,

\(
    \theta' = (\theta_1, \theta_2, \ldots, \theta_p, b)^T \\
    x' = (x_{11}, x_{12}, \ldots, x_{1p}, 1) \\
\)

If we extend to the matrix $X_{n*p}$, add a bias column vector in which each value is 1, $o_{p*1}$ to X,
then we have $X'_{n*(p+1)}$

\(
    X' = cat(X, o) = \begin{pmatrix}
                         x_11 & \ldots & x_{1p} & 1 \\
                         \vdots & \ddots & \vdots & \vdots \\
                         x_n1 & \ldots & x_{np} & 1 \\
                     \end{pmatrix}
\)

=== Norm 2 is Sensitive to Outliers
As we know, we solve $\theta$ for $y=\theta^Tx + b$ by minimizing the loss function,
$L(\theta) = \sum_{i=1}^{n}(\theta^Tx_i - y_i)^2 = ||X\theta - Y||_2^2$. If there is a extreme outlier, then $\theta$ will
be adjusted, and the approximate line, $y=\theta^Tx$, will move close to the outlier to ensure a small loss value. However,
it does not accurately captures the intrinsic pattern of the data. In other words, the outlier will lead to a high loss
if we stick to the real pattern line. Thus, it is required for a loss function that are not sensitive to the outlier.
The norm 1, $L(\theta) = ||X\theta - Y||_1 = \sum_{i=1}^n |\theta^Tx_i - y_i|$ can be one option as it does not exemplify the errors from the outlier compared
the norm 2.

==== Solution 1 - Norm 1 as Loss function
*One weakness is no derivative with respect to x for the function $f(x) = |x|$ will be obtained when $x=0$ in the domain*. However, if we
range x and calculate the derivative for each range,

  \(
  \frac{\partial f(x)}{\partial x} = \left\{
  \begin{array}{ll}
  1, & x > 0 \\
  [-1, 1], & x = 0 \\
  -1, & x < 0\end{array}\right.
  \)

Then the derivative of Norm 1 with respect to $\theta$,
\(
    \frac{\partial L}{\partial \theta^{j}} = \sum_{i=1}^{n} [sgn(\theta^Tx_i - y_i) \frac{\partial (\theta^Tx_i - y_i)}{\partial \theta^j}]
    = \sum_{i=1}^{n} [sgn(\theta^Tx_i - y_i) x_i^j]
\)

Where, $x_i^j$ is the jth value in ith data records, which corresponds the $\theta_j$, and $sgn(z)$ can be seen as below,

\(
    sgn(z) = \left\{
    \begin{array}{ll}
    1, & z > 0 \\
    0, & z = 0 \\
    -1, & z < 0\end{array}\right.
\)

Where $z = \theta^Tx_i - y_i$. Then we use gradient descent formula to update the $\theta^j$,

\(
    \theta^{j'} = \theta^j - \eta \cdot \sum_{i=1}^{n} sgn(\theta^Tx_i - y_i)x_i^j
\)

$\theta^{j'}$ and $\theta$ are scale values. Extending them to a vector, $\theta$, then

\(
    \theta^{'} = \theta - \eta \cdot \sum_{i=1}^{n} sgn(\theta^Tx_i - y_i)x_i
\)

Adapt the sum item into a matrix-vector multiplication (make your hand dirty),

~~~
\(
    \theta^{'} = \theta - \eta \cdot X^T sgn(X\theta - Y)
\)
~~~

$sgn(X\theta - Y)_{n*1}$, apply sgn function for each value in the vector.

==== Solution 2 - Huber Loss
To be less sensitive to outliers and differentiable when $z=0$, we introduce the Huber Loss function, a combination of
Norm 1 (absolute value error) and Norm 2 (quadratic value). See formula,

\(
    H(z) = \left\{
    \begin{array}{ll}
    \delta(|z| - \frac{\delta}{2}), & |z| >= \delta \\
    \frac{z^2}{2}, & |z| < \delta \end{array}\right.
\)

Recall the graph, and describe The Huber loss has a parameter $\delta$ that controls the degree of penalization
for large errors. When the absolute error $|z|$ is smaller than $\delta$, the loss is equivalent to the MSE
loss, while when the absolute error is larger than $\delta$, the loss is equivalent to the MAE loss.

~~~
Huber loss is differentiable at anywhere, even when $|z|=\delta$. Recall $H'(z)$ graph.
~~~

\(
    H'(z) = \left\{
    \begin{array}{ll}
    \delta, & z >= \delta \\
    z, & |z| < \delta \\
    -\delta, & z <= \delta \end{array}\right.
\)

=== Overfitting
This is a *general issue* that not only suits for the linear regression, but we list it here for the introduction of later
Regularization section. The *intuition* is that a model is too complex to learn from data and noise, while gain the knowledge
from the noise is not what we wanna. As a consequence, the model will performs excellent in the training data, however,
worse in unseen data (test data). Here are *causes* that give rise to the overfitting,

- A complex model with too many parameters
- Less training data or unrepresentative data
- Train many iterations that let the model memorize the data and noise
- Use irrelevant or redundant features

~~~
Overfitting means large $\theta$ values. For fitting a data distribution closely, we need a curve with a flexible and
sharp shape that the gradient is large at points in the curve, i.e. $\theta$ is large.
~~~

*Attempts* can be implemented for avoiding the overfitting (simplify the model),

- *Add Regularization.* Will mention in later section.
- *Data Augmentation.* Increase the number of training samples from the existing data for the data diversity. Examples can
be rotation, flipping, cropping in computer vision task.
- *Early Stopping.* Avoid learn too much, and stop at the iteration that the test loss/accuracy increase/decrease.
- *Feature Selection.* Drop irrelevant or redundant features.
- *Cross Validation.* Get a mean accuracy for several models, and they are tested for loop k folds and trained on the rest
the k-1 folds.
- *Ensemble Method.*  Ensemble methods are a technique to combine multiple models to improve the performance and prevent
overfitting. Two popular types of ensemble methods are bagging and boosting. *Bagging* combines the predictions of multiple
models trained on different subsets of the data, while *boosting* combines the predictions of multiple models trained
sequentially on the same dataset.

== Regularization
The *idea* of regularization enables a smaller $\theta$ value than the one got from the loss function for avoiding overfitting.
Then, the optimization task becomes solving $\theta$ for loss function, $||X\theta - Y||_2^2 $, under the constraint
that $\theta$ is not large. We transform the optimization problem with the constraints into one without any constraints
by introducing a penalty, $\lambda Norm(\theta)$ on the loss function,

\(
    L(\theta, \lambda) = ||X\theta - Y||_2^2 + \lambda ||\theta||
\)

Here, $\lambda >= 0$, we adopt Norm 1 (||\theta||), and $L(\theta, \lambda)$ is called *Lagrange Function*. The optimized $\theta^{*}$ will
be obtained by letting the derivative of loss function with respect to $\theta$ equals 0 for minimizing the Lagrange
Function. If $\lambda = 0$, the Lagrange Function equals to the original loss function with no constraints on $\theta$.
IF $\lambda > 0$, then if $|\theta|$ is large, the loss function would be small (0 in the most extreme situation). The
sum value is large. If $\theta$ is small, the loss function would be large. The sum value is also large. So, the ideal
$|\theta|$ is not small and not large.

~~~
{TO DO LIST}{}
Why the Lagrange Function will work here? Find the insight and quantify it under the equation, inequality constraints,
and both.
~~~

=== Lasso Regression
To solve $\theta^{*}$, cost function: $L(\theta) = ||X\theta - Y||_{2}^{2} + \lambda||\theta||$, deviate $L(\theta)$ with $\theta$,

\(
\frac{\partial L(\theta)}{\partial \theta} = 2X^TX\theta - 2X^TY + \lambda sgn(\theta)
\)

where,
\(
    sgn(\theta) =
        \begin{cases}
            1 & \theta_{i} > 0 \\
            0 & \theta_i = 0 \\
            -1 & \theta_i < 0
        \end{cases}
\)

Check the shape of all symbols in the above formula.

\(
X_{n\cdot d}, ~ \theta \in R^{d}, ~ Y_{n \cdot 1}, ~ \lambda \in R, ~ \theta \in R^{d},
\lambda sgn(\theta) \in R^{d}, ~ \frac{\partial J(\theta)}{\partial \theta} \in R^{d}
\)

Let derivation formula as 0, i.e., $\frac{\partial L(\theta)}{\partial \theta} = 0 $

\(
\theta^{\star} = (X^TX)^{-1}(X^TY - \frac{\lambda}{2} sgn(\theta))
\)


However, we have no idea for the $\theta$ in the right part of the close form solution, gradient descent is better to employ here,

~~~
\(
    \theta^{t+1} = \theta^{t} - \alpha \frac{\partial L(\theta^t)}{\partial \theta^t} = \theta^{t} - \alpha [2X^T(X\theta^t - Y) + \lambda sgn(\theta^t)]
\)
~~~

where $\alpha$ is the learning rate, and t is the iteration time. After several iterations, and the convergence condition
is met, then we obtain $\theta^{\star}$. Btw, We can define $L(\theta) = \frac{1}{2}||X\theta - Y||_{2}^{2} + \lambda||\theta||$
to remove the scale, 2, in the update formula.

=== Ridge Regression
To solve $\theta^{*}$, cost function: $$, deviate $L(\theta)$ with $\theta$,
\(
    \frac{\partial L(\theta)}{\partial \theta} = 2(X^T(X\theta - Y) + \lambda\theta)
\)

Let derivation formula as 0, i.e., $\frac{\partial L(\theta)}{\partial \theta} = 0 $

~~~
 \(
    \theta^{*} = (X^TX + \lambda I)^{-1}X^TY
 \)
~~~

where, $I_{p*p}$. Compared to the normal equation from the linear regression without regularization, there is one item,
$\lambda I$ in the parentheses. The update formula for the gradient descent can be seen as follows,

~~~
\(
    \theta^{t+1} = \theta^{t} - \alpha \frac{\partial L(\theta^t)}{\partial \theta^t} = \theta^{t} - 2\alpha [X^T(X\theta^t - Y) + \lambda \theta^t]
\)
~~~

==== Geometry Intuition for Lasso and Ridge Regression
Before we move that far, let us think think a optimization objective with the constraints in geometry. Here is the problem,

\(
    \underset{x, y}{min}( x^2 + y^2 ) \quad \text{subject to,} \quad x + y = 1
\)

This can be summarized as 'one static and one move' problem. *One static* is the constraint in geometry is a line that cross
the point $(0,1)$ and straddles the first and third quadrants with slope value of $-1$. *One move* is the function $x^2 + y^2$.
We mark it as $r^2 = x^2 + y^2$, a circle with a radius $r$. For $r:0 \rightarrow \infty$, the circle is movable.
Besides, the original problem, $\underset{x, y}{min}( x^2 + y^2 )$, is equivalent to minimize $r$, and subject to $x+y-1=0$.
That means the point $(x, y)$ that let $r$ a minimum only lies on the line $x+y-1 = 0$. In other words, the minimum value
$r^{2^*}$ occurs only when the circle is tangent to the this line. In this situation, $r^{2^*} = \frac{1}{2}$, and $x=y=\frac{1}{2}$.
The intuitive effect is a movable circle with a radius $r$ approaches to the line, and the optimal solution is when both

~~~
{Lasso and Ridge Regression}
\(
L(w) = ||Xw - Y||_{2}^{2} + \lambda||w|| \\
L(w) = ||Xw - Y||_{2}^{2} + \lambda||w||_2^2
\)
~~~

The goal is to find the optimal $w^{*}$. The left sub-graph is for Ridge regression and the right one is for Lasso Regression.
Take the left graph as an example, the *blue concentric circles* are a subset of projections of the loss function $L(w) = ||Xw-Y||_2^2$
on the the plane which $w$ lies. Brainstorm for the contour of the loss function and its location on the 3-D space,
$(w_1, w_2, L)$. The *red curve and its inner yellow area* are the constraints, $||W|| <= \delta$. Because we plans to find
the $min L(w)$, which is to find the minimum radius of the blue circus. Under this radius, the circus will be tangent to
the red curve at the *point $w^{*}$*. So is true of Lasso Regression.

~~~
{}{img_left}{../../../images/notes/ml/Linear_Regression/intuition_lasso_ridge.png}{}{500}
~~~

The interesting thing is that the blue curve inclines to get the optimal points with the red curve when on those at the
axis, $w_1 = 0$ or $w_2 = 0$, for Lasso Regression. As some of the coefficient, ${w_i}$ are zero,
the feature will be ignored for the linear regression. Thus, Lasso Regression can drop those unnecessary features that may
be dependent automatically, so *Lasso Regression can be used in the stage of feature selection* instead of removing features
manually by the domain knowledge.

=== Regularization General Form
Below is the formula of the general form for the loss function and regularization,
\(
    L(w) = \frac{1}{2}(w^Tx_i - y_i)^2 + \frac{\lambda}{2}\sum_{j=1}^{p}|w_j|^{q}
\)

For visualization easiness, $w$ for 2-dimensional space is picked.
The contour for $\sum_{j=1}^{2}|w_j|^2$ can be seen as follows,
~~~
{}{img_left}{../images/notes/ml/Linear_Regression/contour_regularization.PNG}{}{800}
~~~

== Summary
This blog is for the linear regression. We start with the Least Square Estimation (LSE). Normal equation and Gradient
Descent are two typical math tools for solving to obtain the optimal $\theta$ for the fitted equation, $y=\theta^Tx + b$.
In normal equation, assumptions like $X^TX$ should be full rank and the number of data points should exceeds the one of
features of X are explored. For training acceleration, Stochastic Gradient Descent (SGD) is summarized.
Then, we solve the problem in another two perspectives, Maximum Likelihood Estimation (MLE), and Maximum a Posterior (MAP).
We also give the comparison between these two perspectives using the math intuition in section MAP. Next, we move forward
to the current issues and solutions in our existing blog are presented. They are no bias, sensitive to outliers, and easy
to overfit. Later, several potential solutions for avoiding overfitting are listed and one of them, add the regularization,
is fully played. With various norm addition on the loss function, Lasso Regression and Ridge Regression are explained in
terms of definition, geometry intuition, and further application (feature selection for Lasso Regression). Besides, the
general form for regularization is given along with their shape.

~~~
We found that maximizing the MLE when the noise follows a Gaussian Distribution, i.e, $P(Y|\theta)$ when
$y_i = \theta^TX + \epsilon, \quad \epsilon \sim \mathcal{N}(0, \sigma^2)$, equals to minimizing the square errors in LSE.
Besides, The minimum least squares error with an L2 regularization term is equivalent to the MAP solution under Gaussian
noise prior. Adding an L1 regularization term is equivalent to MAP solution under the Laplace noise prior, which can be
a practice for those friends who are interested in this problem.
~~~

~~~
{TO DO LIST}{}
Induction for the MAP solution under the laplace prior.
~~~

== Weaknesses of Linear Regression and Their Solutions
- Unable to fit the data closely. 1) Add polynomial features. 2) Add a bias function outside the linear equation. 3) Depth for multi-linear transformation.
- Linearity for the full space. Break this rule by various functions for different spaces.
- No Linearity in the high-dimensional space and hard to learn. Usually, in high-dimensional space, the data is sparse. Dimensionality reduction
may be helpful. PCA and Encoder are two ways for achieving this purpose.

== Implementation

A fake data will be designed. Normal equation, Gradient Descent, and Stochastic Gradient Descent will be tested. Lasso
and Ridge regression are also taken into consideration. Performances under different algorithms and parameters are analyzed.
We plan to implement the above goals using three styles, python from scratch only with numpy, Pytorch, and sklearn package.
The code will be published on the github using google colab. Here, we give the link for different styles.

=== Reference
- Lecture Notes, Prof. Ramin
- [Machine Learning Blackboard Notes and Its Video](https://www.yuque.com/bystander-wg876/yc5f72/mkn2fh)
- Images from Bishop in the book, Pattern Recognition and Machine Learning
- Wikipedia
- chatGPT

==== Thanks a lot for those people who have the open-source spirits. Without them, I can not finish this blog. Feel free to let me know if I made a mistake, you can comment in toMe section on the left panel or pull a request in the github page.
